# @package _global_

defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /model: default.yaml
  - override /data: trainid.yaml

data:
  id_dir: /home/n_masuda/Datasets/diffsynth_5-6/harmor_2oscfree
  ood_dir: /home/n_masuda/Datasets/nsynth-train
  batch_size: 64
  train_type: ood

trainer:
  min_epochs: 1
  max_epochs: 400
  gradient_clip_val: 1.0
  
model:
  _target_: diffsynth.model.EstimatorSynth
  # lr: 0.0001
  # decay_rate: 0.99
  estimator:
    _target_: diffsynth.estimator.MelEstimator
    hidden_size: 512
  l_sched:
    name: switch_1
  sw_loss:
    fft_sizes: [64, 128, 256, 512, 1024, 2048]
  synth_name: harmor_2oscfree
  log_grad: true